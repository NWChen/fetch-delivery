This is Lab Submission 3 for Group 14
Noah Zweben (njz2104), Jessica Vandebon (jav2162), Samantha Wiener (srw2168)

Video of tracking at: https://youtu.be/sAMlOvrxru4

Code for lab 3 is in test.py

When the code starts, the mask video-stream window (black box) appears directly over the color video stream window that you need to click on to select the color, so just drag the mask window over

test.py
----------
test.py uses PiCamera to take photos and then converts them into BGR frames
These frames are loaded into SelectorModule class which handles mask extraction by color
the masked frames are then analyzed for largest blob by area, (findLargestContour handles this)
The initial area and x-centroid of the largest blob are stored. Then, in subsequenct frames, if the x-centroid is over 50 pxls from its initial location, the robot rotates to place the object back within 50 pxls. Once the object is within the 50 pxl of its initial locatiol, the ratio between current area and initial area is calculated. If the ratio is above 1.5, the robot moves backwards. If it is under 0.7, the robot moves forward. We made sure the object was in the center before the area calculations (fwd/bwd movement) were performed to make sure the differences in area were because of distance, not because the object was too far to the side to be out of the robots view.


findLargestContour:
given a masked frame, extracts the largest blob by area to follow as the tracked object

SelectorModule:
This is a class that handles the mask extraction. A frame is loaded in, the user then clicks on two locations within the image. The average color in the region is calculated in HSV, and then a shifter of [30,100,100] is subracted/added to obtain the low/high range to be tracked. (NOTE: this shifter worked well for our lighting, tested in multiple sunny/shady locations, but if you are having trouble masking, we suggest adjusting the sensitivity of the filter)


hand.py
---------
this works very similarly to test.py, only we replaced the area-fwd/bwd relationship with the following gestures:
Move hand to the left: robot turns left
Move hand to the right: robots turns right
Move hand up: robot moves bwd
Move hand down: robots moves fwd:
